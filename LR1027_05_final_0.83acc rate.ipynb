{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "X-Bht-kjbml8",
        "outputId": "a5603f87-8975-417b-f2a0-f982153002c2"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "[0m14s] Installation complete\n",
            "[0m14s] torch: 2.8.0+cu126, CUDA: True\n",
            "==((====))==  Unsloth 2025.10.12: Fast Llama patching. Transformers: 4.57.1.\n",
            "   \\\\   /|    NVIDIA L4. Num GPUs = 1. Max memory: 22.161 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "[0m27s] Model loaded (seq_len=2048)\n",
            "[0m28s] Dataset loaded: 1000000 samples\n",
            "[0m28s] Split - Train: 100000, Val: 10000\n",
            "Data balance - True: 40149 (40.1%), False: 59851\n",
            "[0m35s] Data formatted\n",
            "Sample prompt length: 999 chars\n",
            "[0m42s] LoRA configured - Trainable: 41,943,040 (0.92%)\n",
            "[0m42s] Training config:\n",
            "  Batch size: 2\n",
            "  Grad accumulation: 4\n",
            "  Effective batch: 8\n",
            "  Max steps: 2000\n",
            "  Estimated time: 16.7 minutes\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 100,000 | Num Epochs = 1 | Total steps = 2,000\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 41,943,040 of 8,072,204,288 (0.52% trained)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2001' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2000/2000 7:32:45, Epoch 0.16/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.791200</td>\n",
              "      <td>0.790750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.777600</td>\n",
              "      <td>0.775275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.768900</td>\n",
              "      <td>0.763352</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.746300</td>\n",
              "      <td>0.747242</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.755900</td>\n",
              "      <td>0.733872</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.727000</td>\n",
              "      <td>0.719876</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.720500</td>\n",
              "      <td>0.707727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.698000</td>\n",
              "      <td>0.699022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.685700</td>\n",
              "      <td>0.694581</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1287' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1287/5000 08:47 < 25:22, 2.44 it/s]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Not an error, but LlamaForCausalLM does not accept `num_items_in_batch`.\n",
            "Using gradient accumulation will be very slightly less accurate.\n",
            "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2000/2000 8:07:04, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.791200</td>\n",
              "      <td>0.790750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.777600</td>\n",
              "      <td>0.775275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.768900</td>\n",
              "      <td>0.763352</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.746300</td>\n",
              "      <td>0.747242</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.755900</td>\n",
              "      <td>0.733872</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.727000</td>\n",
              "      <td>0.719876</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.720500</td>\n",
              "      <td>0.707727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.698000</td>\n",
              "      <td>0.699022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.685700</td>\n",
              "      <td>0.694581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.690200</td>\n",
              "      <td>0.693773</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[487m53s] Training completed in 29228s (487.1min)\n",
            "Final train loss: 0.7463714790344238\n",
            "[487m53s] Quick test on 10 samples...\n",
            "  Sample 1: OK Pred=False, Actual=False\n",
            "  Sample 2: OK Pred=True, Actual=True\n",
            "  Sample 3: MISS Pred=False, Actual=True\n",
            "  Sample 4: OK Pred=True, Actual=True\n",
            "  Sample 5: OK Pred=False, Actual=False\n",
            "  Sample 6: OK Pred=True, Actual=True\n",
            "  Sample 7: OK Pred=False, Actual=False\n",
            "  Sample 8: OK Pred=False, Actual=False\n",
            "  Sample 9: OK Pred=False, Actual=False\n",
            "  Sample 10: OK Pred=False, Actual=False\n",
            "[487m57s] Full validation on 500 samples...\n",
            "[490m36s] Validation complete\n",
            "\n",
            "============================================================\n",
            "Validation Accuracy: 0.8200 (410/500)\n",
            "Prediction distribution - True: 38.6%, False: 61.4%\n",
            "Actual distribution     - True: 36.6%, False: 63.4%\n",
            "============================================================\n",
            "\n",
            "EXCELLENT PERFORMANCE\n",
            "[492m39s] Model saved to: ./saved_model_fast\n",
            "[492m41s] Test samples: 10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Predicting: 100%|██████████| 10000/10000 [53:23<00:00,  3.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submission Statistics:\n",
            "  Total: 10000\n",
            "  True: 3918 (39.2%)\n",
            "  False: 6082 (60.8%)\n",
            "\n",
            "Distribution looks reasonable\n",
            "Total runtime: 546.1 minutes\n",
            "Validation accuracy: 0.8200\n",
            "Model saved: ./saved_model_fast\n",
            "Submission file: submission.csv\n",
            "\n",
            "Your Results Summary:\n",
            "  Validation: 0.8200 (EXCELLENT!)\n",
            "  Baseline: 0.726\n",
            "  Status: BEAT BASELINE \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "def print_time(message):\n",
        "    elapsed = time.time() - start_time\n",
        "    print(f\"[{elapsed//60:.0f}m{elapsed%60:.0f}s] {message}\")\n",
        "\n",
        "# 1. INSTALLATION\n",
        "!pip install -q \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "print_time(\"Installation complete\")\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "print_time(f\"torch: {torch.__version__}, CUDA: {torch.cuda.is_available()}\")\n",
        "\n",
        "# 2. Model\n",
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "print_time(f\"Model loaded (seq_len={max_seq_length})\")\n",
        "\n",
        "# 3. LOADING DATASET\n",
        "from datasets import load_dataset\n",
        "\n",
        "full_dataset = load_dataset(\"ad6398/nyu-dl-teach-maths-comp\", split=\"train\")\n",
        "print_time(f\"Dataset loaded: {len(full_dataset)} samples\")\n",
        "\n",
        "shuffled = full_dataset.shuffle(seed=42)\n",
        "train_size = 100000  # Increased from 50000 to improve accuracy\n",
        "val_size = 10000     # Increased from 5000 to improve accuracy\n",
        "\n",
        "train_dataset = shuffled.select(range(train_size))\n",
        "validation_dataset = shuffled.select(range(train_size, train_size + val_size))\n",
        "\n",
        "print_time(f\"Split - Train: {len(train_dataset)}, Val: {len(validation_dataset)}\")\n",
        "\n",
        "true_count = sum(1 for ex in train_dataset if ex[\"is_correct\"])\n",
        "false_count = len(train_dataset) - true_count\n",
        "print(f\"Data balance - True: {true_count} ({100*true_count/len(train_dataset):.1f}%), False: {false_count}\")\n",
        "\n",
        "# 4. FORMATTING DATA\n",
        "\n",
        "training_prompt = \"\"\"Check if the solution is correct.\n",
        "\n",
        "Question: {}\n",
        "Solution: {}\n",
        "Answer: {}\n",
        "\n",
        "Correct?\n",
        "Output:\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    texts = []\n",
        "    for q, s, a, o in zip(examples[\"question\"], examples[\"solution\"],\n",
        "                          examples[\"answer\"], examples[\"is_correct\"]):\n",
        "        output_str = \"True\" if o else \"False\"\n",
        "        text = training_prompt.format(str(q).strip(), str(s).strip(), str(a).strip())\n",
        "        texts.append(text + f\" {output_str}{EOS_TOKEN}\")\n",
        "    return {\"text\": texts}\n",
        "\n",
        "formatted_train = train_dataset.map(formatting_prompts_func, batched=True,\n",
        "                                     remove_columns=train_dataset.column_names)\n",
        "formatted_val = validation_dataset.map(formatting_prompts_func, batched=True,\n",
        "                                        remove_columns=validation_dataset.column_names)\n",
        "\n",
        "print_time(\"Data formatted\")\n",
        "print(f\"Sample prompt length: {len(formatted_train[0]['text'])} chars\")\n",
        "\n",
        "# ==================== 5. CONFIGURING LORA ====================\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha = 32,\n",
        "    lora_dropout = 0.05,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 42,\n",
        ")\n",
        "\n",
        "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total = sum(p.numel() for p in model.parameters())\n",
        "print_time(f\"LoRA configured - Trainable: {trainable:,} ({100*trainable/total:.2f}%)\")\n",
        "\n",
        "# 6. TRAINING SETUP\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "import os\n",
        "\n",
        "os.makedirs(\"outputs\", exist_ok=True)\n",
        "\n",
        "batch_size = 2\n",
        "grad_accum = 4\n",
        "effective_batch = batch_size * grad_accum\n",
        "max_steps = 2000  # Increased from 1000 to train more with larger dataset\n",
        "\n",
        "estimated_time = max_steps * 0.5 / 60\n",
        "print_time(f\"Training config:\")\n",
        "print(f\"  Batch size: {batch_size}\")\n",
        "print(f\"  Grad accumulation: {grad_accum}\")\n",
        "print(f\"  Effective batch: {effective_batch}\")\n",
        "print(f\"  Max steps: {max_steps}\")\n",
        "print(f\"  Estimated time: {estimated_time:.1f} minutes\")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = formatted_train,\n",
        "    eval_dataset = formatted_val,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = batch_size,\n",
        "        gradient_accumulation_steps = grad_accum,\n",
        "        warmup_steps = 50,\n",
        "        max_steps = max_steps,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not torch.cuda.is_bf16_supported(),\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "        logging_steps = 100,\n",
        "        eval_strategy = \"steps\",\n",
        "        eval_steps = 200,\n",
        "        save_strategy = \"steps\",\n",
        "        save_steps = 1000,  # Adjusted to match larger max_steps\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"cosine\",\n",
        "        seed = 42,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "# 7. Starting training\n",
        "\n",
        "trainer_stats = trainer.train()\n",
        "\n",
        "train_time = trainer_stats.metrics['train_runtime']\n",
        "print_time(f\"Training completed in {train_time:.0f}s ({train_time/60:.1f}min)\")\n",
        "print(f\"Final train loss: {trainer_stats.metrics.get('train_loss', 'N/A')}\")\n",
        "\n",
        "# 8. VALIDATION\n",
        "\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "inference_prompt = \"\"\"Check if the solution is correct.\n",
        "\n",
        "Question: {}\n",
        "Solution: {}\n",
        "Answer: {}\n",
        "\n",
        "Correct?\n",
        "Output:\"\"\"\n",
        "\n",
        "def parse_output(text):\n",
        "    try:\n",
        "        part = text.split(\"Output:\")[-1].strip().lower()\n",
        "        if \"true\" in part[:100]:\n",
        "            return True\n",
        "        elif \"false\" in part[:100]:\n",
        "            return False\n",
        "        return \"true\" in part\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "print_time(\"Quick test on 10 samples...\")\n",
        "for i in range(10):\n",
        "    ex = validation_dataset[i]\n",
        "    prompt = inference_prompt.format(ex[\"question\"], str(ex[\"solution\"]), str(ex[\"answer\"]))\n",
        "    inputs = tokenizer([prompt], return_tensors=\"pt\", truncation=True, max_length=max_seq_length).to(\"cuda\")\n",
        "    outputs = model.generate(**inputs, max_new_tokens=10, use_cache=True, temperature=0.1)\n",
        "    pred = parse_output(tokenizer.decode(outputs[0]))\n",
        "    match = \"OK\" if pred == ex[\"is_correct\"] else \"MISS\"\n",
        "    print(f\"  Sample {i+1}: {match} Pred={pred}, Actual={ex['is_correct']}\")\n",
        "\n",
        "print_time(\"Full validation on 500 samples...\")\n",
        "correct = 0\n",
        "total = 500\n",
        "predictions_val = []\n",
        "actuals_val = []\n",
        "\n",
        "for i in range(total):\n",
        "    ex = validation_dataset[i]\n",
        "    prompt = inference_prompt.format(ex[\"question\"], str(ex[\"solution\"]), str(ex[\"answer\"]))\n",
        "    inputs = tokenizer([prompt], return_tensors=\"pt\", truncation=True, max_length=max_seq_length).to(\"cuda\")\n",
        "    outputs = model.generate(**inputs, max_new_tokens=10, use_cache=True, temperature=0.1)\n",
        "    pred = parse_output(tokenizer.decode(outputs[0]))\n",
        "    predictions_val.append(pred)\n",
        "    actuals_val.append(ex[\"is_correct\"])\n",
        "\n",
        "    if pred == ex[\"is_correct\"]:\n",
        "        correct += 1\n",
        "\n",
        "accuracy = correct / total\n",
        "pred_true_ratio = sum(predictions_val) / len(predictions_val)\n",
        "actual_true_ratio = sum(actuals_val) / len(actuals_val)\n",
        "\n",
        "print_time(\"Validation complete\")\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Validation Accuracy: {accuracy:.4f} ({correct}/{total})\")\n",
        "print(f\"Prediction distribution - True: {100*pred_true_ratio:.1f}%, False: {100*(1-pred_true_ratio):.1f}%\")\n",
        "print(f\"Actual distribution     - True: {100*actual_true_ratio:.1f}%, False: {100*(1-actual_true_ratio):.1f}%\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "if accuracy < 0.55:\n",
        "    print(\"\\nWARNING: ACCURACY < 0.55\")\n",
        "elif accuracy < 0.726:\n",
        "    print(\"\\nWARNING: ACCURACY < 0.726 (baseline)\")\n",
        "elif accuracy < 0.80:\n",
        "    print(\"\\nGOOD: ABOVE BASELINE\")\n",
        "else:\n",
        "    print(\"\\nEXCELLENT PERFORMANCE\")\n",
        "\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    save_path = \"/content/drive/MyDrive/llama3_math_verifier_fast\"\n",
        "except:\n",
        "    save_path = \"./saved_model_fast\"\n",
        "\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)\n",
        "print_time(f\"Model saved to: {save_path}\")\n",
        "\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "test_dataset = load_dataset(\"ad6398/nyu-dl-teach-maths-comp\", split=\"test\")\n",
        "print_time(f\"Test samples: {len(test_dataset)}\")\n",
        "\n",
        "predictions = []\n",
        "failed_count = 0\n",
        "\n",
        "for i in tqdm(range(len(test_dataset)), desc=\"Predicting\"):\n",
        "    ex = test_dataset[i]\n",
        "\n",
        "    prompt = inference_prompt.format(ex[\"question\"], str(ex[\"solution\"]), str(ex[\"answer\"]))\n",
        "\n",
        "    try:\n",
        "        inputs = tokenizer(\n",
        "            [prompt],\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            max_length=max_seq_length,\n",
        "            padding=False\n",
        "        ).to(\"cuda\")\n",
        "\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=10,\n",
        "            use_cache=True,\n",
        "            temperature=0.1\n",
        "        )\n",
        "        pred = parse_output(tokenizer.decode(outputs[0]))\n",
        "    except Exception as e:\n",
        "        failed_count += 1\n",
        "        if failed_count <= 3:\n",
        "            print(f\"\\nWarning: Sample {i} failed: {str(e)[:80]}\")\n",
        "        pred = False\n",
        "\n",
        "    predictions.append(pred)\n",
        "\n",
        "if failed_count > 0:\n",
        "    print(f\"\\nTotal failed samples: {failed_count} (defaulted to False)\")\n",
        "\n",
        "submission = pd.DataFrame({'ID': range(len(predictions)), 'is_correct': predictions})\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "\n",
        "true_count = sum(predictions)\n",
        "true_ratio = true_count / len(predictions)\n",
        "\n",
        "print(f\"Submission Statistics:\")\n",
        "print(f\"  Total: {len(predictions)}\")\n",
        "print(f\"  True: {true_count} ({100*true_ratio:.1f}%)\")\n",
        "print(f\"  False: {len(predictions)-true_count} ({100*(1-true_ratio):.1f}%)\")\n",
        "\n",
        "if true_ratio > 0.95 or true_ratio < 0.05:\n",
        "    print(f\"\\nWARNING: Heavily skewed to {'True' if true_ratio > 0.5 else 'False'}\")\n",
        "else:\n",
        "    print(f\"\\nDistribution looks reasonable\")\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print(f\"Total runtime: {total_time/60:.1f} minutes\")\n",
        "print(f\"Validation accuracy: {accuracy:.4f}\")\n",
        "print(f\"Model saved: {save_path}\")\n",
        "print(f\"Submission file: submission.csv\")\n",
        "print(f\"\\nYour Results Summary:\")\n",
        "print(f\"  Validation: {accuracy:.4f} {'(EXCELLENT!)' if accuracy >= 0.80 else ''}\")\n",
        "print(f\"  Baseline: 0.726\")\n",
        "print(f\"  Status: {'BEAT BASELINE ' if accuracy >= 0.726 else 'BELOW BASELINE'}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('submission.csv')\n"
      ],
      "metadata": {
        "id": "f7EffQFmIpKT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "4b49d8c1-f43d-493f-b988-a56ee603643c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_51e561a1-311d-4e5a-9b7d-96b7454486c5\", \"submission.csv\", 104986)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "history_visible": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GpClbrF-B2xG"
      },
      "outputs": [],
      "source": [
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps \"xformers<0.0.26\" \"trl<0.9.0\" \"peft<0.12.0\" \"accelerate<0.32.0\" \"bitsandbytes<0.44.0\" \"transformers<4.43.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HY2AoNCu0rlk"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "print(\"Loading model...\")\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Meta-Llama-3.1-8B\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "print(f\"Model loaded successfully. Max sequence length: {max_seq_length}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjmRBdGy0uZi"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "full_dataset = load_dataset(\"ad6398/nyu-dl-teach-maths-comp\", split=\"train\")\n",
        "print(f\"Total training samples: {len(full_dataset)}\")\n",
        "\n",
        "shuffled_dataset = full_dataset.shuffle(seed=42)\n",
        "train_size = int(len(shuffled_dataset) * 0.9)\n",
        "train_dataset = shuffled_dataset.select(range(train_size))\n",
        "validation_dataset = shuffled_dataset.select(range(train_size, len(shuffled_dataset)))\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)}\")\n",
        "print(f\"Validation samples: {len(validation_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lt0EAVT9046e"
      },
      "outputs": [],
      "source": [
        "training_prompt = \"\"\"Analyze the math problem and solution. Output 'True' if correct, 'False' if incorrect.\n",
        "\n",
        "Question: {}\n",
        "Solution: {}\n",
        "Answer: {}\n",
        "Output: {}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    questions = examples[\"question\"]\n",
        "    solutions = examples[\"solution\"]\n",
        "    answers = examples[\"answer\"]\n",
        "    outputs = examples[\"is_correct\"]\n",
        "    texts = []\n",
        "    for question, solution, answer, output in zip(questions, solutions, answers, outputs):\n",
        "        text = training_prompt.format(\n",
        "            question,\n",
        "            str(solution),\n",
        "            str(answer),\n",
        "            str(output)\n",
        "        ) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return {\"text\": texts}\n",
        "\n",
        "print(\"\\nFormatting datasets...\")\n",
        "formatted_train_dataset = train_dataset.map(formatting_prompts_func, batched=True)\n",
        "formatted_val_dataset = validation_dataset.map(formatting_prompts_func, batched=True)\n",
        "\n",
        "print(\"Sample formatted text:\")\n",
        "print(formatted_train_dataset[0][\"text\"][:300])\n",
        "print(f\"\\nDataset columns: {formatted_train_dataset.column_names}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOc5OoHb07FQ"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha = 32,\n",
        "    lora_dropout = 0.05,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 42,\n",
        ")\n",
        "\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "all_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Trainable params: {trainable_params:,} ({100 * trainable_params / all_params:.2f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVIMgD5z0-5K"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, EarlyStoppingCallback\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = formatted_train_dataset,\n",
        "    eval_dataset = formatted_val_dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        per_device_eval_batch_size = 4,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 10,\n",
        "        num_train_epochs = 2,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not torch.cuda.is_bf16_supported(),\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "        logging_steps = 50,\n",
        "        eval_strategy = \"steps\",\n",
        "        eval_steps = 200,\n",
        "        save_strategy = \"steps\",\n",
        "        save_steps = 200,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"cosine\",\n",
        "        seed = 42,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\",\n",
        "        load_best_model_at_end = True,\n",
        "        metric_for_best_model = \"loss\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(f\"Total steps: ~{len(formatted_train_dataset) * 2 // (2 * 4)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlWcQrqH1BFA"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0ib3tQT1EG9"
      },
      "outputs": [],
      "source": [
        "\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "inference_prompt = \"\"\"Analyze the math problem and solution. Output 'True' if correct, 'False' if incorrect.\n",
        "\n",
        "Question: {}\n",
        "Solution: {}\n",
        "Answer: {}\n",
        "Output: \"\"\"\n",
        "\n",
        "def parse_output(response_text):\n",
        "    try:\n",
        "        output_part = response_text.split(\"Output:\")[-1].strip().lower()\n",
        "        if \"true\" in output_part[:20]:\n",
        "            return True\n",
        "        elif \"false\" in output_part[:20]:\n",
        "            return False\n",
        "        return \"true\" in output_part\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "correct = 0\n",
        "total = min(100, len(validation_dataset))\n",
        "\n",
        "print(f\"\\nEvaluating on {total} validation samples...\")\n",
        "for i in range(total):\n",
        "    example = validation_dataset[i]\n",
        "    question = example[\"question\"]\n",
        "    solution = example[\"solution\"]\n",
        "    answer = example[\"answer\"]\n",
        "\n",
        "    prompt = inference_prompt.format(question, str(solution), str(answer))\n",
        "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(**inputs, max_new_tokens=10, use_cache=True, temperature=0.1)\n",
        "    response_text = tokenizer.batch_decode(outputs)[0]\n",
        "\n",
        "    prediction = parse_output(response_text)\n",
        "    actual = example[\"is_correct\"]\n",
        "\n",
        "    if prediction == actual:\n",
        "        correct += 1\n",
        "\n",
        "    if i < 5:\n",
        "        print(f\"\\n--- Sample {i+1} ---\")\n",
        "        print(f\"Predicted: {prediction}, Actual: {actual}, Match: {prediction == actual}\")\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Validation Accuracy: {accuracy:.4f} ({correct}/{total})\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "if accuracy < 0.55:\n",
        "    print(\"WARNING: Accuracy below 0.55 - consider retraining with different hyperparameters\")\n",
        "elif accuracy < 0.726:\n",
        "    print(\"INFO: Accuracy below baseline (0.726) - room for improvement\")\n",
        "elif accuracy >= 0.80:\n",
        "    print(\"EXCELLENT: Accuracy above 0.80!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JeDSRDGG1HIu"
      },
      "outputs": [],
      "source": [
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    save_path = \"/content/drive/MyDrive/llama3_math_verifier\"\n",
        "except:\n",
        "    save_path = \"./saved_model\"\n",
        "\n",
        "import os\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "print(f\"Saving model to {save_path}...\")\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kvudLN8zsqO"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "test_dataset = load_dataset(\"ad6398/nyu-dl-teach-maths-comp\", split=\"test\")\n",
        "print(f\"Test samples: {len(test_dataset)}\")\n",
        "\n",
        "predictions = []\n",
        "true_count = 0\n",
        "false_count = 0\n",
        "\n",
        "for example in tqdm(test_dataset, desc=\"Generating predictions\"):\n",
        "    question = example[\"question\"]\n",
        "    solution = example[\"solution\"]\n",
        "    answer = example[\"answer\"]\n",
        "\n",
        "    prompt = inference_prompt.format(question, str(solution), str(answer))\n",
        "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(**inputs, max_new_tokens=10, use_cache=True, temperature=0.1)\n",
        "    response_text = tokenizer.batch_decode(outputs)[0]\n",
        "\n",
        "    prediction = parse_output(response_text)\n",
        "    predictions.append(prediction)\n",
        "\n",
        "    if prediction:\n",
        "        true_count += 1\n",
        "    else:\n",
        "        false_count += 1\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    'ID': range(len(predictions)),\n",
        "    'is_correct': predictions\n",
        "})\n",
        "\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "\n",
        "print(f\"Total predictions: {len(predictions)}\")\n",
        "print(f\"True predictions: {true_count} ({100*true_count/len(predictions):.2f}%)\")\n",
        "print(f\"False predictions: {false_count} ({100*false_count/len(predictions):.2f}%)\")\n",
        "print(f\"\\nSubmission file 'submission.csv' created!\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "print(\"\\nFirst 10 predictions:\")\n",
        "print(submission.head(10))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
